{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ff1469",
   "metadata": {},
   "source": [
    "# CheckPoint Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After watching this video below, you will be able to:\n",
    "\n",
    "#https://www.youtube.com/watch?v=YY5skv756pc\n",
    "\n",
    "#1.1) Write a function to Get and parse html content from a Wikipedia page\n",
    "\n",
    "#1.2) Write a function to Extract article title\n",
    "\n",
    "#1.3) Write a function to Extract article text for each paragraph with their respective\n",
    "\n",
    "#headings. Map those headings to their respective paragraphs in the dictionary.\n",
    "\n",
    "#1.4) Write a function to collect every link that redirects to another Wikipedia page\n",
    "\n",
    "#1.5) Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
    "\n",
    "#1.6) Test the last function on a Wikipedia page of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f389f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: OpenAI\n",
      "Article Text:\n",
      "Redirect Links: ['/wiki/Friendly_AI', '/wiki/AI_control_problem', '/wiki/YC_research', '/wiki/DeepMind', '/wiki/Fiduciary_duty', '/wiki/Y_Combinator_(company)', '/wiki/Stripe_(company)', '/wiki/Y_Combinator_(company)', '/wiki/Intelligence_explosion', '/wiki/Amazon.com', '/wiki/DeepMind', '/wiki/ImageNet_Large_Scale_Visual_Recognition_Challenge', '/wiki/Dendi_(Dota_player)', '/wiki/Explainable_AI', '/wiki/Domain_randomization', '/wiki/Application_programming_interface', '/wiki/Upvotes', '/wiki/Parameter_(machine_learning)', '/wiki/Orders_of_magnitude', '/wiki/Application_programming_interface', '/wiki/Autocompletion', '/wiki/ISSN_(identifier)', '/wiki/Mercury_News', '/wiki/ArXiv_(identifier)', '/wiki/ArXiv_(identifier)', '/wiki/ArXiv_(identifier)', '/wiki/ArXiv_(identifier)', '/wiki/ArXiv_(identifier)', '/wiki/Training,_validation,_and_test_sets', '/wiki/Residual_network', '/wiki/Intelligence_explosion']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve HTML content. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def extract_article_title(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title_element = soup.find('h1', {'id': 'firstHeading'})\n",
    "    if title_element:\n",
    "        return title_element.text.strip()\n",
    "    else:\n",
    "        raise Exception(\"Failed to extract article title.\")\n",
    "\n",
    "\n",
    "def extract_article_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content_element = soup.find('div', {'id': 'mw-content-text'})\n",
    "    if content_element:\n",
    "        paragraphs = content_element.find_all('p', recursive=False)\n",
    "        text_map = {}\n",
    "\n",
    "        current_heading = None\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.find('span', {'class': 'mw-headline'}):\n",
    "                current_heading = paragraph.text.strip()\n",
    "                text_map[current_heading] = []\n",
    "            elif current_heading:\n",
    "                text_map[current_heading].append(paragraph.text.strip())\n",
    "\n",
    "        return text_map\n",
    "    else:\n",
    "        raise Exception(\"Failed to extract article text.\")\n",
    "\n",
    "\n",
    "def collect_redirect_links(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    link_elements = soup.find_all('a', {'class': 'mw-redirect'})\n",
    "    redirect_links = []\n",
    "    for link in link_elements:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/wiki/'):\n",
    "            redirect_links.append(href)\n",
    "\n",
    "    return redirect_links\n",
    "\n",
    "\n",
    "def process_wikipedia_page(url):\n",
    "    html = get_html_content(url)\n",
    "    title = extract_article_title(html)\n",
    "    text_map = extract_article_text(html)\n",
    "    redirect_links = collect_redirect_links(html)\n",
    "\n",
    "    result = {\n",
    "        'title': title,\n",
    "        'text': text_map,\n",
    "        'redirect_links': redirect_links\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "wikipedia_link = \"https://en.wikipedia.org/wiki/OpenAI\"\n",
    "result = process_wikipedia_page(wikipedia_link)\n",
    "print(\"Article Title:\", result['title'])\n",
    "print(\"Article Text:\")\n",
    "for heading, paragraphs in result['text'].items():\n",
    "    print(heading)\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph)\n",
    "    print(\"---\")\n",
    "print(\"Redirect Links:\", result['redirect_links'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dcd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
